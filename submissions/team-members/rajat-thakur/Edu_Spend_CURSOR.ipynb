{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "--------------------------------------------------\n",
      "Number of rows: 907\n",
      "Number of columns: 12\n",
      "\n",
      "Columns:\n",
      "['Country', 'City', 'University', 'Program', 'Level', 'Duration_Years', 'Tuition_USD', 'Living_Cost_Index', 'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD', 'Exchange_Rate']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>University</th>\n",
       "      <th>Program</th>\n",
       "      <th>Level</th>\n",
       "      <th>Duration_Years</th>\n",
       "      <th>Tuition_USD</th>\n",
       "      <th>Living_Cost_Index</th>\n",
       "      <th>Rent_USD</th>\n",
       "      <th>Visa_Fee_USD</th>\n",
       "      <th>Insurance_USD</th>\n",
       "      <th>Exchange_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55400</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2200</td>\n",
       "      <td>160</td>\n",
       "      <td>1500</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK</td>\n",
       "      <td>London</td>\n",
       "      <td>Imperial College London</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Master</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41200</td>\n",
       "      <td>75.8</td>\n",
       "      <td>1800</td>\n",
       "      <td>485</td>\n",
       "      <td>800</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>University of Toronto</td>\n",
       "      <td>Business Analytics</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38500</td>\n",
       "      <td>72.5</td>\n",
       "      <td>1600</td>\n",
       "      <td>235</td>\n",
       "      <td>900</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>University of Melbourne</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42000</td>\n",
       "      <td>71.2</td>\n",
       "      <td>1400</td>\n",
       "      <td>450</td>\n",
       "      <td>650</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Munich</td>\n",
       "      <td>Technical University of Munich</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>500</td>\n",
       "      <td>70.5</td>\n",
       "      <td>1100</td>\n",
       "      <td>75</td>\n",
       "      <td>550</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country       City                      University  \\\n",
       "0        USA  Cambridge              Harvard University   \n",
       "1         UK     London         Imperial College London   \n",
       "2     Canada    Toronto           University of Toronto   \n",
       "3  Australia  Melbourne         University of Melbourne   \n",
       "4    Germany     Munich  Technical University of Munich   \n",
       "\n",
       "                  Program   Level  Duration_Years  Tuition_USD  \\\n",
       "0        Computer Science  Master             2.0        55400   \n",
       "1            Data Science  Master             1.0        41200   \n",
       "2      Business Analytics  Master             2.0        38500   \n",
       "3             Engineering  Master             2.0        42000   \n",
       "4  Mechanical Engineering  Master             2.0          500   \n",
       "\n",
       "   Living_Cost_Index  Rent_USD  Visa_Fee_USD  Insurance_USD  Exchange_Rate  \n",
       "0               83.5      2200           160           1500           1.00  \n",
       "1               75.8      1800           485            800           0.79  \n",
       "2               72.5      1600           235            900           1.35  \n",
       "3               71.2      1400           450            650           1.52  \n",
       "4               70.5      1100            75            550           0.92  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('/Users/rajatthakur/Desktop/SuperDataScienceML/CollaborationProjects/Edu-spend/International_Education_Costs.csv')\n",
    "\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Number of rows:\", df.shape[0])\n",
    "print(\"Number of columns:\", df.shape[1])\n",
    "print(\"\\nColumns:\")\n",
    "print(df.columns.to_list())\n",
    "\n",
    "\n",
    "# Display first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature Engineering\n",
    "First, youâ€™ll want to create a new column for TCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TCA'] = (\n",
    "    df['Tuition_USD'] * df['Duration_Years'] +\n",
    "    df['Rent_USD'] * 12 * df['Duration_Years'] +\n",
    "    df['Visa_Fee_USD'] +\n",
    "    df['Insurance_USD'] * df['Duration_Years']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>University</th>\n",
       "      <th>Program</th>\n",
       "      <th>Level</th>\n",
       "      <th>Duration_Years</th>\n",
       "      <th>Tuition_USD</th>\n",
       "      <th>Living_Cost_Index</th>\n",
       "      <th>Rent_USD</th>\n",
       "      <th>Visa_Fee_USD</th>\n",
       "      <th>Insurance_USD</th>\n",
       "      <th>Exchange_Rate</th>\n",
       "      <th>TCA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55400</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2200</td>\n",
       "      <td>160</td>\n",
       "      <td>1500</td>\n",
       "      <td>1.00</td>\n",
       "      <td>166760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK</td>\n",
       "      <td>London</td>\n",
       "      <td>Imperial College London</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Master</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41200</td>\n",
       "      <td>75.8</td>\n",
       "      <td>1800</td>\n",
       "      <td>485</td>\n",
       "      <td>800</td>\n",
       "      <td>0.79</td>\n",
       "      <td>64085.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>University of Toronto</td>\n",
       "      <td>Business Analytics</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38500</td>\n",
       "      <td>72.5</td>\n",
       "      <td>1600</td>\n",
       "      <td>235</td>\n",
       "      <td>900</td>\n",
       "      <td>1.35</td>\n",
       "      <td>117435.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>University of Melbourne</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42000</td>\n",
       "      <td>71.2</td>\n",
       "      <td>1400</td>\n",
       "      <td>450</td>\n",
       "      <td>650</td>\n",
       "      <td>1.52</td>\n",
       "      <td>119350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Munich</td>\n",
       "      <td>Technical University of Munich</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>500</td>\n",
       "      <td>70.5</td>\n",
       "      <td>1100</td>\n",
       "      <td>75</td>\n",
       "      <td>550</td>\n",
       "      <td>0.92</td>\n",
       "      <td>28575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>France</td>\n",
       "      <td>Strasbourg</td>\n",
       "      <td>University of Strasbourg</td>\n",
       "      <td>Data Analytics</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>70.2</td>\n",
       "      <td>1000</td>\n",
       "      <td>99</td>\n",
       "      <td>850</td>\n",
       "      <td>0.92</td>\n",
       "      <td>33799.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Nilai</td>\n",
       "      <td>USIM</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6800</td>\n",
       "      <td>50.5</td>\n",
       "      <td>400</td>\n",
       "      <td>120</td>\n",
       "      <td>400</td>\n",
       "      <td>4.65</td>\n",
       "      <td>36120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Al-Ahsa</td>\n",
       "      <td>King Faisal University</td>\n",
       "      <td>Information Systems</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4200</td>\n",
       "      <td>64.2</td>\n",
       "      <td>600</td>\n",
       "      <td>200</td>\n",
       "      <td>800</td>\n",
       "      <td>3.75</td>\n",
       "      <td>24600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>USA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>University of Washington</td>\n",
       "      <td>Software Development</td>\n",
       "      <td>PhD</td>\n",
       "      <td>5.0</td>\n",
       "      <td>50000</td>\n",
       "      <td>77.8</td>\n",
       "      <td>2000</td>\n",
       "      <td>160</td>\n",
       "      <td>1500</td>\n",
       "      <td>1.00</td>\n",
       "      <td>377660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>UK</td>\n",
       "      <td>Nottingham</td>\n",
       "      <td>University of Nottingham</td>\n",
       "      <td>Data Engineering</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34000</td>\n",
       "      <td>61.2</td>\n",
       "      <td>800</td>\n",
       "      <td>485</td>\n",
       "      <td>800</td>\n",
       "      <td>0.79</td>\n",
       "      <td>89285.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country        City                      University  \\\n",
       "0             USA   Cambridge              Harvard University   \n",
       "1              UK      London         Imperial College London   \n",
       "2          Canada     Toronto           University of Toronto   \n",
       "3       Australia   Melbourne         University of Melbourne   \n",
       "4         Germany      Munich  Technical University of Munich   \n",
       "..            ...         ...                             ...   \n",
       "902        France  Strasbourg        University of Strasbourg   \n",
       "903      Malaysia       Nilai                            USIM   \n",
       "904  Saudi Arabia     Al-Ahsa          King Faisal University   \n",
       "905           USA     Seattle        University of Washington   \n",
       "906            UK  Nottingham        University of Nottingham   \n",
       "\n",
       "                    Program     Level  Duration_Years  Tuition_USD  \\\n",
       "0          Computer Science    Master             2.0        55400   \n",
       "1              Data Science    Master             1.0        41200   \n",
       "2        Business Analytics    Master             2.0        38500   \n",
       "3               Engineering    Master             2.0        42000   \n",
       "4    Mechanical Engineering    Master             2.0          500   \n",
       "..                      ...       ...             ...          ...   \n",
       "902          Data Analytics    Master             2.0         4000   \n",
       "903        Computer Science  Bachelor             3.0         6800   \n",
       "904     Information Systems    Master             2.0         4200   \n",
       "905    Software Development       PhD             5.0        50000   \n",
       "906        Data Engineering    Master             2.0        34000   \n",
       "\n",
       "     Living_Cost_Index  Rent_USD  Visa_Fee_USD  Insurance_USD  Exchange_Rate  \\\n",
       "0                 83.5      2200           160           1500           1.00   \n",
       "1                 75.8      1800           485            800           0.79   \n",
       "2                 72.5      1600           235            900           1.35   \n",
       "3                 71.2      1400           450            650           1.52   \n",
       "4                 70.5      1100            75            550           0.92   \n",
       "..                 ...       ...           ...            ...            ...   \n",
       "902               70.2      1000            99            850           0.92   \n",
       "903               50.5       400           120            400           4.65   \n",
       "904               64.2       600           200            800           3.75   \n",
       "905               77.8      2000           160           1500           1.00   \n",
       "906               61.2       800           485            800           0.79   \n",
       "\n",
       "          TCA  \n",
       "0    166760.0  \n",
       "1     64085.0  \n",
       "2    117435.0  \n",
       "3    119350.0  \n",
       "4     28575.0  \n",
       "..        ...  \n",
       "902   33799.0  \n",
       "903   36120.0  \n",
       "904   24600.0  \n",
       "905  377660.0  \n",
       "906   89285.0  \n",
       "\n",
       "[907 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Select Features and Target\n",
    "Choose which columns you want to use as features (X) and set TCA as your target (y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['TCA', 'Tuition_USD', 'Visa_Fee_USD', 'Insurance_USD', 'Rent_USD'], axis=1)\n",
    "y = df['TCA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>University</th>\n",
       "      <th>Program</th>\n",
       "      <th>Level</th>\n",
       "      <th>Duration_Years</th>\n",
       "      <th>Living_Cost_Index</th>\n",
       "      <th>Exchange_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>83.5</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK</td>\n",
       "      <td>London</td>\n",
       "      <td>Imperial College London</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Master</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.8</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>University of Toronto</td>\n",
       "      <td>Business Analytics</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>University of Melbourne</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.2</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Munich</td>\n",
       "      <td>Technical University of Munich</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.5</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>France</td>\n",
       "      <td>Strasbourg</td>\n",
       "      <td>University of Strasbourg</td>\n",
       "      <td>Data Analytics</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.2</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Nilai</td>\n",
       "      <td>USIM</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.5</td>\n",
       "      <td>4.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Al-Ahsa</td>\n",
       "      <td>King Faisal University</td>\n",
       "      <td>Information Systems</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.2</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>USA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>University of Washington</td>\n",
       "      <td>Software Development</td>\n",
       "      <td>PhD</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.8</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>UK</td>\n",
       "      <td>Nottingham</td>\n",
       "      <td>University of Nottingham</td>\n",
       "      <td>Data Engineering</td>\n",
       "      <td>Master</td>\n",
       "      <td>2.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country        City                      University  \\\n",
       "0             USA   Cambridge              Harvard University   \n",
       "1              UK      London         Imperial College London   \n",
       "2          Canada     Toronto           University of Toronto   \n",
       "3       Australia   Melbourne         University of Melbourne   \n",
       "4         Germany      Munich  Technical University of Munich   \n",
       "..            ...         ...                             ...   \n",
       "902        France  Strasbourg        University of Strasbourg   \n",
       "903      Malaysia       Nilai                            USIM   \n",
       "904  Saudi Arabia     Al-Ahsa          King Faisal University   \n",
       "905           USA     Seattle        University of Washington   \n",
       "906            UK  Nottingham        University of Nottingham   \n",
       "\n",
       "                    Program     Level  Duration_Years  Living_Cost_Index  \\\n",
       "0          Computer Science    Master             2.0               83.5   \n",
       "1              Data Science    Master             1.0               75.8   \n",
       "2        Business Analytics    Master             2.0               72.5   \n",
       "3               Engineering    Master             2.0               71.2   \n",
       "4    Mechanical Engineering    Master             2.0               70.5   \n",
       "..                      ...       ...             ...                ...   \n",
       "902          Data Analytics    Master             2.0               70.2   \n",
       "903        Computer Science  Bachelor             3.0               50.5   \n",
       "904     Information Systems    Master             2.0               64.2   \n",
       "905    Software Development       PhD             5.0               77.8   \n",
       "906        Data Engineering    Master             2.0               61.2   \n",
       "\n",
       "     Exchange_Rate  \n",
       "0             1.00  \n",
       "1             0.79  \n",
       "2             1.35  \n",
       "3             1.52  \n",
       "4             0.92  \n",
       "..             ...  \n",
       "902           0.92  \n",
       "903           4.65  \n",
       "904           3.75  \n",
       "905           1.00  \n",
       "906           0.79  \n",
       "\n",
       "[907 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Encode Categorical Variables: \n",
    "Random Forests canâ€™t handle string categories directly, so we need to encode them. The most common way is One-Hot Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Select categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the columns to a file\n",
    "X_encoded.columns.to_list()\n",
    "pd.Series(X_encoded.columns).to_csv(\"model_columns.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Ready for Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,      # Number of trees in the forest\n",
    "    random_state=42        # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation (optional, but recommended):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 9894.01\n",
      "RMSE: 16888.06\n",
      "R^2: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R^2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual_TCA  Predicted_TCA\n",
      "0      24040.0       23206.85\n",
      "1      28310.0       24337.05\n",
      "2     215850.0      190129.40\n",
      "3      58500.0       47968.83\n",
      "4      29350.0       55937.00\n",
      "5      13310.0       17936.90\n",
      "6      23650.0       21943.05\n",
      "7     107590.0      105752.50\n",
      "8      33799.0       33895.13\n",
      "9      24420.0       27084.30\n",
      "10     30675.0       24481.64\n",
      "11    377660.0      386030.00\n",
      "12     60820.0       59358.10\n",
      "13     55620.0       50953.00\n",
      "14     79035.0       76530.25\n",
      "15     52560.0       50120.20\n",
      "16    408660.0      405860.00\n",
      "17     38810.0       36987.43\n",
      "18     40320.0       48608.15\n",
      "19     40875.0       30120.74\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted values\n",
    "results = pd.DataFrame({\n",
    "    'Actual_TCA': y_test.values,\n",
    "    'Predicted_TCA': y_pred\n",
    "})\n",
    "\n",
    "# Display the first few rows\n",
    "print(results.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_profile = {\n",
    "    'Country': 'USA',\n",
    "    'City': 'Boston',\n",
    "    'University': 'MIT',\n",
    "    'Program': 'Data Science',\n",
    "    'Level': 'Master',\n",
    "    'Duration_Years': 2,\n",
    "    'Living_Cost_Index': 85.0,\n",
    "    'Rent_USD': 2500,\n",
    "    'Visa_Fee_USD': 160,\n",
    "    'Insurance_USD': 1500,\n",
    "    'Exchange_Rate': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted TCA: $167660.00\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "new_df = pd.DataFrame([new_profile])\n",
    "\n",
    "# One-hot encode (align columns with training data)\n",
    "new_df_encoded = pd.get_dummies(new_df)\n",
    "new_df_encoded = new_df_encoded.reindex(columns=X_encoded.columns, fill_value=0)\n",
    "\n",
    "# Predict TCA\n",
    "predicted_tca = rf.predict(new_df_encoded)\n",
    "print(f\"Predicted TCA: ${predicted_tca[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature Importance: which features are most influential in predicting TCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Feature  Importance\n",
      "69                 Country_USA    0.375562\n",
      "2                Exchange_Rate    0.133731\n",
      "0               Duration_Years    0.114852\n",
      "1345              Level_Master    0.072013\n",
      "5            Country_Australia    0.070762\n",
      "68                  Country_UK    0.068172\n",
      "1            Living_Cost_Index    0.048708\n",
      "12              Country_Canada    0.030578\n",
      "1346                 Level_PhD    0.011242\n",
      "529             City_Singapore    0.010250\n",
      "1269  Program_Computer Science    0.008524\n",
      "56           Country_Singapore    0.006739\n",
      "67                 Country_UAE    0.004977\n",
      "1344            Level_Bachelor    0.004347\n",
      "34             Country_Ireland    0.004305\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances and feature names\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_encoded.columns\n",
    "\n",
    "# Create a DataFrame for easy sorting and viewing\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort by importance descending and show top 15\n",
    "top_features = feat_imp_df.sort_values(by='Importance', ascending=False).head(15)\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "Country_USA (0.375):\n",
    "Whether the program is in the USA is the most influential factor in predicting TCA. This makes sense, as US education costs are often much higher than in other countries.\n",
    "\n",
    "Exchange_Rate (0.134):\n",
    "The exchange rate has a significant impact, affecting the conversion of local costs to USD.\n",
    "\n",
    "Duration_Years (0.110):\n",
    "The length of the program is also important, as longer programs naturally cost more.\n",
    "\n",
    "Level_Master (0.070):\n",
    "Whether the program is a Masterâ€™s degree is also a key factor.\n",
    "\n",
    "What does this mean?\n",
    "\n",
    "Your model is heavily influenced by the country (especially the USA), the exchange rate, and the duration of the program.\n",
    "These results are logical and suggest your data is being used sensibly by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Hyperparameter Tuning (RandomizedSearchCV Example): This will help you find the best parameters for your Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "Best parameters found: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n",
      "MAE (tuned): 21400.708450215992\n",
      "RMSE (tuned): 16888.060154482297\n",
      "R^2 (tuned): 0.8586908869472908\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit to training data\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found:\", rf_random.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_rf = rf_random.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "print(\"MAE (tuned):\", mean_absolute_error(y_test, y_pred_best))\n",
    "print(\"RMSE (tuned):\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R^2 (tuned):\", r2_score(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/bin/python -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (XGBoost): 9225.625520475618\n",
      "RMSE (XGBoost): 16888.060154482297\n",
      "R^2 (XGBoost): 0.964075767850752\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Train XGBoost regressor\n",
    "xgb = XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "print(\"MAE (XGBoost):\", mean_absolute_error(y_test, y_pred_xgb))\n",
    "print(\"RMSE (XGBoost):\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R^2 (XGBoost):\", r2_score(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Actual_TCA  Predicted_TCA_XGB\n",
      "0     24040.0       24314.378906\n",
      "1     28310.0       21836.056641\n",
      "2    215850.0      196507.984375\n",
      "3     58500.0       47074.191406\n",
      "4     29350.0       71652.789062\n",
      "5     13310.0       13716.552734\n",
      "6     23650.0       27203.396484\n",
      "7    107590.0      115703.031250\n",
      "8     33799.0       32385.943359\n",
      "9     24420.0       24232.083984\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "results_xgb = pd.DataFrame({\n",
    "    'Actual_TCA': y_test.values,\n",
    "    'Predicted_TCA_XGB': y_pred_xgb\n",
    "})\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(results_xgb.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Actual_TCA  Predicted_TCA_XGB  Predicted_TCA_RF\n",
      "0     24040.0       24314.378906          23206.85\n",
      "1     28310.0       21836.056641          24337.05\n",
      "2    215850.0      196507.984375         190129.40\n",
      "3     58500.0       47074.191406          47968.83\n",
      "4     29350.0       71652.789062          55937.00\n",
      "5     13310.0       13716.552734          17936.90\n",
      "6     23650.0       27203.396484          21943.05\n",
      "7    107590.0      115703.031250         105752.50\n",
      "8     33799.0       32385.943359          33895.13\n",
      "9     24420.0       24232.083984          27084.30\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for comparison\n",
    "results_compare = pd.DataFrame({\n",
    "    'Actual_TCA': y_test.values,\n",
    "    'Predicted_TCA_XGB': y_pred_xgb,\n",
    "    'Predicted_TCA_RF': y_pred\n",
    "})\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(results_compare.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.6; total time=   1.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.6; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.6; total time=   2.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=1.0; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.6; total time=   2.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=1.0; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.6; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, n_estimators=300, subsample=0.6; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, n_estimators=300, subsample=0.6; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, n_estimators=300, subsample=0.6; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=10, n_estimators=300, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=10, n_estimators=300, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=10, n_estimators=300, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.6; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.6; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.6; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=10, n_estimators=100, subsample=1.0; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=10, n_estimators=100, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=10, n_estimators=100, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   1.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   1.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "Best parameters found: {'subsample': 0.6, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "MAE (tuned): 9724.361405928057\n",
      "RMSE (tuned): 16888.060154482297\n",
      "R^2 (tuned): 0.9638865516121785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_random.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found:\", xgb_random.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_xgb_tuned = xgb_random.predict(X_test)\n",
    "print(\"MAE (tuned):\", mean_absolute_error(y_test, y_pred_xgb_tuned))\n",
    "print(\"RMSE (tuned):\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R^2 (tuned):\", r2_score(y_test, y_pred_xgb_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Actual_TCA  Predicted_TCA_XGB  Predicted_TCA_RF  Predicted_TCA_XGB_Tuned\n",
      "0     24040.0       24314.378906          23206.85             23226.193359\n",
      "1     28310.0       21836.056641          24337.05             23790.230469\n",
      "2    215850.0      196507.984375         190129.40            197560.328125\n",
      "3     58500.0       47074.191406          47968.83             49277.796875\n",
      "4     29350.0       71652.789062          55937.00             62570.437500\n",
      "5     13310.0       13716.552734          17936.90             17793.029297\n",
      "6     23650.0       27203.396484          21943.05             25673.412109\n",
      "7    107590.0      115703.031250         105752.50            115260.578125\n",
      "8     33799.0       32385.943359          33895.13             32885.480469\n",
      "9     24420.0       24232.083984          27084.30             23321.582031\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for comparison\n",
    "results_compare = pd.DataFrame({\n",
    "    'Actual_TCA': y_test.values,\n",
    "    'Predicted_TCA_XGB': y_pred_xgb,\n",
    "    'Predicted_TCA_RF': y_pred,\n",
    "    'Predicted_TCA_XGB_Tuned': y_pred_xgb_tuned\n",
    "})\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(results_compare.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Random Forest run with MAE: 21400.71, RMSE: 30838.01, R2: 0.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Use your best_rf model from RandomizedSearchCV\n",
    "with mlflow.start_run(run_name=\"RandomForest Tuned\"):\n",
    "    # Predict on the test set\n",
    "    y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_rf.get_params())\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(best_rf, \"model\")\n",
    "\n",
    "    print(f\"Logged Random Forest run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n"
     ]
    }
   ],
   "source": [
    "  import sklearn\n",
    "  print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/rajatthakur/miniforge3/envs/14Junemlflow-clean-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['0'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"/Users/rajatthakur/Desktop/SuperDataScienceML/CollaborationProjects/Edu-spend/model_columns.csv\")\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run with MAE: 9724.36, RMSE: 15589.63, R2: 0.964\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Tuned\"):\n",
    "    # Use best parameters from RandomizedSearchCV\n",
    "    best_params = xgb_random.best_params_\n",
    "    xgb_final = XGBRegressor(**best_params, random_state=42)\n",
    "    xgb_final.fit(X_train, y_train)\n",
    "    y_pred_xgb_tuned = xgb_final.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_final, \"model\")\n",
    "\n",
    "    print(f\"Logged run with MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "14Junemlflow-clean-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
