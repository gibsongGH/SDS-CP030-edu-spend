{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c75ec4",
   "metadata": {},
   "source": [
    "# EduSpend: Final Models Development\n",
    "## Affordability Classifier & University Clustering\n",
    "\n",
    "**Project:** EduSpend - Global Higher-Education Cost Analytics & Planning  \n",
    "**Author:** yan-cotta  \n",
    "**Date:** June 27, 2025  \n",
    "**Phase:** Final Models - Classifier & Clustering  \n",
    "\n",
    "### Notebook Overview\n",
    "This notebook implements the final two models required for the EduSpend project:\n",
    "1. **Affordability Classifier**: RandomForestClassifier to predict affordability tiers\n",
    "2. **University Clustering**: KMeans clustering to group universities by cost patterns\n",
    "\n",
    "### Goals\n",
    "1. Load and prepare the education cost dataset\n",
    "2. Create and evaluate an affordability classification model\n",
    "3. Implement university clustering based on cost features\n",
    "4. Save the final labeled dataset for downstream applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cff23",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We'll import the necessary libraries for data processing, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    f1_score, \n",
    "    confusion_matrix,\n",
    "    silhouette_score\n",
    ")\n",
    "\n",
    "# Import plotting libraries\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fbd2d",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Dataset\n",
    "\n",
    "We'll load the education cost dataset and examine its structure to understand the available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd1de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# First, try to load the cleaned dataset, fallback to original if not available\n",
    "try:\n",
    "    df = pd.read_csv('data/cleaned_education_costs.csv')\n",
    "    print(\"âœ… Loaded cleaned_education_costs.csv\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df = pd.read_csv('data/International_Education_Costs.csv')\n",
    "        print(\"âœ… Loaded International_Education_Costs.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ No dataset found. Please ensure the data file is in the data/ directory.\")\n",
    "        raise\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"\\nðŸ“Š Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nðŸ“‹ Columns: {list(df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nðŸ” First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df0822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"ðŸ” Missing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nðŸ“Š Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nðŸ“ˆ Dataset Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba77ea",
   "metadata": {},
   "source": [
    "## Step 3: Data Preparation and Feature Engineering\n",
    "\n",
    "We'll prepare the data by calculating TCA (if not present) and creating the affordability tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a58661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TCA (Total Cost of Attendance) if not present\n",
    "if 'TCA' not in df.columns:\n",
    "    print(\"ðŸ”§ Creating TCA column...\")\n",
    "    # Calculate TCA as sum of all cost components\n",
    "    cost_columns = ['Tuition_USD', 'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD']\n",
    "    \n",
    "    # Handle missing values by filling with 0\n",
    "    for col in cost_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Calculate TCA\n",
    "    df['TCA'] = df['Tuition_USD'].fillna(0)\n",
    "    \n",
    "    if 'Rent_USD' in df.columns:\n",
    "        # Multiply rent by 12 for annual cost\n",
    "        df['TCA'] += df['Rent_USD'].fillna(0) * 12\n",
    "    \n",
    "    if 'Visa_Fee_USD' in df.columns:\n",
    "        df['TCA'] += df['Visa_Fee_USD'].fillna(0)\n",
    "    \n",
    "    if 'Insurance_USD' in df.columns:\n",
    "        df['TCA'] += df['Insurance_USD'].fillna(0)\n",
    "    \n",
    "    print(f\"âœ… TCA column created with range: ${df['TCA'].min():,.0f} - ${df['TCA'].max():,.0f}\")\n",
    "else:\n",
    "    print(\"âœ… TCA column already exists\")\n",
    "\n",
    "# Display TCA statistics\n",
    "print(f\"\\nðŸ“Š TCA Statistics:\")\n",
    "print(f\"Mean TCA: ${df['TCA'].mean():,.0f}\")\n",
    "print(f\"Median TCA: ${df['TCA'].median():,.0f}\")\n",
    "print(f\"Standard Deviation: ${df['TCA'].std():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create affordability tiers based on TCA quantiles\n",
    "if 'affordability_tier' not in df.columns and 'Affordability_Tier' not in df.columns:\n",
    "    print(\"ðŸ”§ Creating affordability_tier column...\")\n",
    "    \n",
    "    # Calculate quantiles\n",
    "    q33 = df['TCA'].quantile(0.33)\n",
    "    q67 = df['TCA'].quantile(0.67)\n",
    "    \n",
    "    print(f\"ðŸ’° TCA Quantiles:\")\n",
    "    print(f\"33rd percentile: ${q33:,.0f}\")\n",
    "    print(f\"67th percentile: ${q67:,.0f}\")\n",
    "    \n",
    "    # Create affordability tiers\n",
    "    def assign_affordability_tier(tca):\n",
    "        if tca <= q33:\n",
    "            return 'Low'\n",
    "        elif tca <= q67:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'High'\n",
    "    \n",
    "    df['affordability_tier'] = df['TCA'].apply(assign_affordability_tier)\n",
    "    \n",
    "    print(\"âœ… Affordability tiers created\")\n",
    "elif 'Affordability_Tier' in df.columns:\n",
    "    df['affordability_tier'] = df['Affordability_Tier']\n",
    "    print(\"âœ… Using existing Affordability_Tier column\")\n",
    "else:\n",
    "    print(\"âœ… affordability_tier column already exists\")\n",
    "\n",
    "# Display affordability tier distribution\n",
    "print(f\"\\nðŸ“Š Affordability Tier Distribution:\")\n",
    "tier_counts = df['affordability_tier'].value_counts()\n",
    "for tier, count in tier_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{tier}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c79be",
   "metadata": {},
   "source": [
    "## Step 4: Affordability Classification Model\n",
    "\n",
    "We'll create a RandomForestClassifier to predict affordability tiers based on the available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for classification\n",
    "print(\"ðŸ”§ Preparing features for classification...\")\n",
    "\n",
    "# Select relevant features for classification (excluding TCA since it's used to create the target)\n",
    "feature_columns = []\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "\n",
    "# Add categorical features\n",
    "for col in ['Country', 'City', 'Program', 'Level']:\n",
    "    if col in df.columns:\n",
    "        categorical_features.append(col)\n",
    "        feature_columns.append(col)\n",
    "\n",
    "# Add numerical features (excluding TCA)\n",
    "for col in ['Duration_Years', 'Living_Cost_Index', 'Exchange_Rate', 'Tuition_USD', 'Rent_USD', 'Visa_Fee_USD', 'Insurance_USD']:\n",
    "    if col in df.columns:\n",
    "        numerical_features.append(col)\n",
    "        feature_columns.append(col)\n",
    "\n",
    "print(f\"ðŸ“‹ Selected Features:\")\n",
    "print(f\"Categorical: {categorical_features}\")\n",
    "print(f\"Numerical: {numerical_features}\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = df[feature_columns].copy()\n",
    "y = df['affordability_tier'].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Feature Matrix Shape: {X.shape}\")\n",
    "print(f\"ðŸ“Š Target Variable Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle categorical variables with one-hot encoding\n",
    "print(\"ðŸ”§ Encoding categorical variables...\")\n",
    "\n",
    "# Fill missing values\n",
    "for col in numerical_features:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "for col in categorical_features:\n",
    "    X[col] = X[col].fillna('Unknown')\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features, prefix=categorical_features)\n",
    "\n",
    "print(f\"âœ… Encoded feature matrix shape: {X_encoded.shape}\")\n",
    "print(f\"ðŸ“‹ Final features: {len(X_encoded.columns)} total features\")\n",
    "\n",
    "# Display first few columns\n",
    "print(f\"\\nðŸ” Sample encoded features: {list(X_encoded.columns[:10])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for training and testing\n",
    "print(\"ðŸ”§ Splitting data into train and test sets...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Training set: {X_train.shape}\")\n",
    "print(f\"ðŸ“Š Test set: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution in train and test sets\n",
    "print(f\"\\nðŸ“Š Training set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "\n",
    "print(f\"\\nðŸ“Š Test set class distribution:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e440b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the RandomForestClassifier\n",
    "print(\"ðŸš€ Training RandomForestClassifier...\")\n",
    "\n",
    "# Initialize the classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"âœ… RandomForestClassifier training completed!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test)\n",
    "\n",
    "print(f\"ðŸ“Š Predictions completed for {len(y_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "print(\"ðŸ“Š Evaluating Affordability Classifier Performance\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"ðŸŽ¯ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"ðŸŽ¯ Macro F1-Score: {f1_macro:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nðŸ“‹ Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Cross-validation scores\n",
    "print(f\"\\nðŸ”„ Cross-Validation Performance:\")\n",
    "cv_scores = cross_val_score(rf_classifier, X_encoded, y, cv=5, scoring='accuracy')\n",
    "print(f\"CV Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "\n",
    "cv_f1_scores = cross_val_score(rf_classifier, X_encoded, y, cv=5, scoring='f1_macro')\n",
    "print(f\"CV Macro F1: {cv_f1_scores.mean():.4f} Â± {cv_f1_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6dfef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"ðŸ” Feature Importance Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_encoded.columns,\n",
    "    'importance': rf_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Display top 15 most important features\n",
    "print(\"ðŸ† Top 15 Most Important Features:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances for Affordability Classification')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Affordability Classification Model Complete!\")\n",
    "print(f\"Final Performance: Accuracy = {accuracy:.4f}, Macro F1 = {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c143df",
   "metadata": {},
   "source": [
    "## Step 5: University Clustering Analysis\n",
    "\n",
    "We'll implement KMeans clustering to group universities/destinations into distinct cost archetypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ffccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for clustering\n",
    "print(\"ðŸ”§ Preparing features for university clustering...\")\n",
    "\n",
    "# Select numerical cost and index features for clustering\n",
    "clustering_features = []\n",
    "for col in ['Tuition_USD', 'Rent_USD', 'Living_Cost_Index', 'Insurance_USD', 'Visa_Fee_USD']:\n",
    "    if col in df.columns:\n",
    "        clustering_features.append(col)\n",
    "\n",
    "print(f\"ðŸ“‹ Selected clustering features: {clustering_features}\")\n",
    "\n",
    "# Create clustering dataset\n",
    "X_cluster = df[clustering_features].copy()\n",
    "\n",
    "# Handle missing values\n",
    "for col in clustering_features:\n",
    "    X_cluster[col] = X_cluster[col].fillna(X_cluster[col].median())\n",
    "\n",
    "print(f\"ðŸ“Š Clustering dataset shape: {X_cluster.shape}\")\n",
    "print(f\"\\nðŸ“ˆ Clustering features summary:\")\n",
    "print(X_cluster.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24219d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features for clustering\n",
    "print(\"ðŸ”§ Scaling features for clustering...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_cluster_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_cluster_scaled_df = pd.DataFrame(X_cluster_scaled, columns=clustering_features)\n",
    "\n",
    "print(f\"âœ… Features scaled successfully\")\n",
    "print(f\"ðŸ“Š Scaled features shape: {X_cluster_scaled_df.shape}\")\n",
    "\n",
    "# Check scaled data statistics\n",
    "print(f\"\\nðŸ“ˆ Scaled features summary:\")\n",
    "print(X_cluster_scaled_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b26aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of clusters using elbow method\n",
    "print(\"ðŸ” Determining optimal number of clusters...\")\n",
    "\n",
    "# Test different numbers of clusters\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_cluster_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    sil_score = silhouette_score(X_cluster_scaled, kmeans.labels_)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    \n",
    "    print(f\"k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={sil_score:.4f}\")\n",
    "\n",
    "# Plot elbow curve and silhouette scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Elbow curve\n",
    "ax1.plot(k_range, inertias, 'bo-')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method for Optimal k')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Silhouette scores\n",
    "ax2.plot(k_range, silhouette_scores, 'ro-')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score vs Number of Clusters')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k based on silhouette score\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "best_silhouette = max(silhouette_scores)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Optimal number of clusters: {optimal_k} (Silhouette Score: {best_silhouette:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3239e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform final clustering with optimal k\n",
    "print(f\"ðŸš€ Performing KMeans clustering with k={optimal_k}...\")\n",
    "\n",
    "# Use k=5 as requested in the prompt (or optimal_k if different)\n",
    "final_k = 5  # As requested in the prompt\n",
    "if optimal_k != final_k:\n",
    "    print(f\"ðŸ“ Note: Using k={final_k} as requested (optimal was k={optimal_k})\")\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=final_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Calculate final silhouette score\n",
    "final_silhouette_score = silhouette_score(X_cluster_scaled, cluster_labels)\n",
    "\n",
    "print(f\"âœ… Clustering completed!\")\n",
    "print(f\"ðŸ“Š Final Silhouette Score: {final_silhouette_score:.4f}\")\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "df['cost_cluster'] = cluster_labels\n",
    "\n",
    "# Display cluster distribution\n",
    "print(f\"\\nðŸ“Š Cluster Distribution:\")\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    percentage = (count / len(cluster_labels)) * 100\n",
    "    print(f\"Cluster {cluster}: {count} universities ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "print(\"ðŸ” Analyzing Cost Cluster Characteristics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate cluster centers in original scale\n",
    "cluster_centers_scaled = kmeans_final.cluster_centers_\n",
    "cluster_centers_original = scaler.inverse_transform(cluster_centers_scaled)\n",
    "\n",
    "# Create cluster analysis dataframe\n",
    "cluster_analysis = pd.DataFrame(cluster_centers_original, columns=clustering_features)\n",
    "cluster_analysis.index = [f'Cluster {i}' for i in range(final_k)]\n",
    "\n",
    "print(\"ðŸ’° Cluster Centers (Average Values):\")\n",
    "print(cluster_analysis.round(0))\n",
    "\n",
    "# Analyze cluster characteristics by calculating statistics for each cluster\n",
    "print(f\"\\nðŸ“Š Detailed Cluster Analysis:\")\n",
    "for cluster_id in range(final_k):\n",
    "    cluster_data = df[df['cost_cluster'] == cluster_id]\n",
    "    cluster_size = len(cluster_data)\n",
    "    avg_tca = cluster_data['TCA'].mean()\n",
    "    \n",
    "    print(f\"\\nðŸ·ï¸ Cluster {cluster_id} (n={cluster_size}):\")\n",
    "    print(f\"   Average TCA: ${avg_tca:,.0f}\")\n",
    "    \n",
    "    # Most common countries in this cluster\n",
    "    if 'Country' in cluster_data.columns:\n",
    "        top_countries = cluster_data['Country'].value_counts().head(3)\n",
    "        print(f\"   Top Countries: {', '.join([f'{country} ({count})' for country, count in top_countries.items()])}\")\n",
    "    \n",
    "    # Most common programs in this cluster\n",
    "    if 'Program' in cluster_data.columns:\n",
    "        top_programs = cluster_data['Program'].value_counts().head(3)\n",
    "        print(f\"   Top Programs: {', '.join([f'{program} ({count})' for program, count in top_programs.items()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26891bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters\n",
    "print(\"ðŸ“Š Creating cluster visualizations...\")\n",
    "\n",
    "# Create subplots for cluster visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: TCA vs Living Cost Index colored by cluster\n",
    "scatter1 = axes[0,0].scatter(df['Living_Cost_Index'], df['TCA'], c=df['cost_cluster'], \n",
    "                            cmap='viridis', alpha=0.7, s=50)\n",
    "axes[0,0].set_xlabel('Living Cost Index')\n",
    "axes[0,0].set_ylabel('Total Cost of Attendance (USD)')\n",
    "axes[0,0].set_title('Clusters: TCA vs Living Cost Index')\n",
    "plt.colorbar(scatter1, ax=axes[0,0])\n",
    "\n",
    "# Plot 2: Tuition vs Rent colored by cluster\n",
    "if 'Tuition_USD' in df.columns and 'Rent_USD' in df.columns:\n",
    "    scatter2 = axes[0,1].scatter(df['Tuition_USD'], df['Rent_USD'], c=df['cost_cluster'], \n",
    "                                cmap='viridis', alpha=0.7, s=50)\n",
    "    axes[0,1].set_xlabel('Tuition (USD)')\n",
    "    axes[0,1].set_ylabel('Monthly Rent (USD)')\n",
    "    axes[0,1].set_title('Clusters: Tuition vs Rent')\n",
    "    plt.colorbar(scatter2, ax=axes[0,1])\n",
    "\n",
    "# Plot 3: Cluster size distribution\n",
    "cluster_counts = df['cost_cluster'].value_counts().sort_index()\n",
    "axes[1,0].bar(cluster_counts.index, cluster_counts.values, color='skyblue')\n",
    "axes[1,0].set_xlabel('Cluster ID')\n",
    "axes[1,0].set_ylabel('Number of Universities')\n",
    "axes[1,0].set_title('Cluster Size Distribution')\n",
    "\n",
    "# Plot 4: Average TCA by cluster\n",
    "avg_tca_by_cluster = df.groupby('cost_cluster')['TCA'].mean()\n",
    "axes[1,1].bar(avg_tca_by_cluster.index, avg_tca_by_cluster.values, color='lightcoral')\n",
    "axes[1,1].set_xlabel('Cluster ID')\n",
    "axes[1,1].set_ylabel('Average TCA (USD)')\n",
    "axes[1,1].set_title('Average TCA by Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Cluster analysis completed!\")\n",
    "print(f\"Final Silhouette Score: {final_silhouette_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecb18e",
   "metadata": {},
   "source": [
    "## Step 6: Save Final Labeled Dataset\n",
    "\n",
    "We'll save the complete dataset with both affordability tiers and cost clusters for downstream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataset with all labels\n",
    "print(\"ðŸ’¾ Preparing final labeled dataset...\")\n",
    "\n",
    "# Ensure we have all required columns\n",
    "required_columns = ['affordability_tier', 'cost_cluster']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        print(f\"âŒ Missing column: {col}\")\n",
    "    else:\n",
    "        print(f\"âœ… Column present: {col}\")\n",
    "\n",
    "# Display final dataset summary\n",
    "print(f\"\\nðŸ“Š Final Dataset Summary:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "\n",
    "# Show the new columns we added\n",
    "print(f\"\\nðŸ·ï¸ New Labels Added:\")\n",
    "print(f\"Affordability Tiers: {df['affordability_tier'].value_counts().to_dict()}\")\n",
    "print(f\"Cost Clusters: {df['cost_cluster'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# Display sample of final dataset\n",
    "print(f\"\\nðŸ” Sample of final labeled dataset:\")\n",
    "display_columns = ['Country', 'City', 'Program', 'Level', 'TCA', 'affordability_tier', 'cost_cluster']\n",
    "available_display_columns = [col for col in display_columns if col in df.columns]\n",
    "print(df[available_display_columns].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de04288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final labeled dataset\n",
    "output_filename = 'data/final_labeled_data.csv'\n",
    "\n",
    "try:\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\"âœ… Final labeled dataset saved to: {output_filename}\")\n",
    "    print(f\"ðŸ“Š Saved {len(df)} records with {len(df.columns)} columns\")\n",
    "    \n",
    "    # Verify the saved file\n",
    "    saved_df = pd.read_csv(output_filename)\n",
    "    print(f\"\\nðŸ” Verification - Loaded saved file:\")\n",
    "    print(f\"Shape: {saved_df.shape}\")\n",
    "    print(f\"Required columns present: {all(col in saved_df.columns for col in required_columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving dataset: {e}\")\n",
    "    print(\"Attempting to save to current directory...\")\n",
    "    try:\n",
    "        df.to_csv('final_labeled_data.csv', index=False)\n",
    "        print(\"âœ… Saved to current directory as 'final_labeled_data.csv'\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Failed to save: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4f5e4",
   "metadata": {},
   "source": [
    "## Summary and Results\n",
    "\n",
    "This notebook successfully implemented both required models for the EduSpend project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cafc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary of results\n",
    "print(\"ðŸŽ¯ FINAL MODELS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nðŸ¤– 1. AFFORDABILITY CLASSIFIER (RandomForestClassifier):\")\n",
    "print(f\"   âœ… Model Type: RandomForestClassifier\")\n",
    "print(f\"   âœ… Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   âœ… Macro F1-Score: {f1_macro:.4f}\")\n",
    "print(f\"   âœ… Features Used: {len(X_encoded.columns)} features\")\n",
    "print(f\"   âœ… Classes: {sorted(df['affordability_tier'].unique())}\")\n",
    "\n",
    "print(f\"\\nðŸ”— 2. UNIVERSITY CLUSTERING (KMeans):\")\n",
    "print(f\"   âœ… Algorithm: KMeans Clustering\")\n",
    "print(f\"   âœ… Number of Clusters: {final_k}\")\n",
    "print(f\"   âœ… Silhouette Score: {final_silhouette_score:.4f}\")\n",
    "print(f\"   âœ… Features Used: {len(clustering_features)} cost features\")\n",
    "print(f\"   âœ… Clustering Features: {clustering_features}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ 3. FINAL DATASET:\")\n",
    "print(f\"   âœ… Total Records: {len(df):,}\")\n",
    "print(f\"   âœ… Total Features: {len(df.columns)}\")\n",
    "print(f\"   âœ… New Labels Added: affordability_tier, cost_cluster\")\n",
    "print(f\"   âœ… Output File: {output_filename}\")\n",
    "\n",
    "print(f\"\\nðŸ† PROJECT STATUS: COMPLETE\")\n",
    "print(f\"   âœ… All required models implemented successfully\")\n",
    "print(f\"   âœ… Classification model trained and evaluated\")\n",
    "print(f\"   âœ… Clustering analysis completed\")\n",
    "print(f\"   âœ… Final labeled dataset saved\")\n",
    "print(f\"   âœ… Ready for deployment and application development\")\n",
    "\n",
    "print(f\"\\nðŸŽ“ BUSINESS IMPACT:\")\n",
    "print(f\"   ðŸ“Š Students can now get affordability predictions with {accuracy*100:.1f}% accuracy\")\n",
    "print(f\"   ðŸŽ¯ Universities grouped into {final_k} distinct cost archetypes\")\n",
    "print(f\"   ðŸ’¡ Cost structure insights available for {len(df)} international programs\")\n",
    "print(f\"   ðŸŒ Global education cost patterns identified and quantified\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
